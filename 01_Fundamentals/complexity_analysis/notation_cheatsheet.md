# ðŸ“˜ Complexity Notation Cheatsheet

Understanding **complexity notations** is fundamental for algorithm analysis.
This cheatsheet provides **definitions, intuition, visual aids, and comparisons** for the most common asymptotic notations.

---

## ðŸ”¹ Why Do We Need Notation?

When analyzing algorithms, exact runtime (like `3nÂ² + 2n + 7`) is **too detailed**.
We only care about how the algorithm **scales with input size $n$**.

Asymptotic notation provides a **mathematical shorthand** for growth rates.

---

## ðŸ”¹ Common Notations

### 1. **Big-O (O) â€” Upper Bound**

* Describes the **worst-case growth rate**.
* Ensures algorithm wonâ€™t grow faster than this rate.

**Formal Definition:**

$$
f(n) = O(g(n)) \quad \text{if} \quad \exists \, c, n_0 \, : \, f(n) \leq c \cdot g(n) \quad \forall n \geq n_0
$$

âœ… Guarantees performance will **not exceed** this bound.

**Example:** Insertion sort = $O(n^2)$.

---

### 2. **Big-Omega (Î©) â€” Lower Bound**

* Describes the **best-case growth rate**.
* Algorithm always takes at least this much time.

**Formal Definition:**

$$
f(n) = \Omega(g(n)) \quad \text{if} \quad \exists \, c, n_0 \, : \, f(n) \geq c \cdot g(n) \quad \forall n \geq n_0
$$

**Example:** Insertion sort = $\Omega(n)$ (already sorted input).

---

### 3. **Big-Theta (Î˜) â€” Tight Bound**

* Represents both **upper and lower bound**.
* Algorithm always runs at this rate (asymptotically).

**Formal Definition:**

$$
f(n) = \Theta(g(n)) \quad \iff \quad f(n) = O(g(n)) \, \wedge \, f(n) = \Omega(g(n))
$$

**Example:** Merge sort = $\Theta(n \log n)$.

---

### 4. **Little-o (o) â€” Strict Upper Bound**

* Growth rate is **strictly less than** another function.

$$
f(n) = o(g(n)) \quad \text{if} \quad \lim_{n \to \infty} \frac{f(n)}{g(n)} = 0
$$

**Example:**
$n = o(n^2)$ â†’ linear grows slower than quadratic.

---

### 5. **Little-omega (Ï‰) â€” Strict Lower Bound**

* Growth rate is **strictly greater than** another function.

$$
f(n) = \omega(g(n)) \quad \text{if} \quad \lim_{n \to \infty} \frac{f(n)}{g(n)} = \infty
$$

**Example:**
$n^2 = \omega(n)$.

---

## ðŸ”¹ Visual Aid: Growth Comparison

```
Growth Rates (as n â†’ âˆž):

O(1)     <   O(log n)   <   O(n)   <   O(n log n)   <   O(n^2)   <   O(2^n)   <   O(n!)
```

ASCII chart:

```
    Complexity    Growth Curve
    --------------------------------
    O(1)          constant: -----
    O(log n)      logarithmic: __/
    O(n)          linear: /
    O(n log n)    n log n: /`
    O(n^2)        quadratic: /â€¾
    O(2^n)        exponential: /|
    O(n!)         factorial: /||
```

---

## ðŸ”¹ Summary Table

| Notation | Meaning            | Bound Type | Example                |
| -------- | ------------------ | ---------- | ---------------------- |
| **O**    | Upper bound        | Worst-case | QuickSort = O(nÂ²)      |
| **Î©**    | Lower bound        | Best-case  | QuickSort = Î©(n log n) |
| **Î˜**    | Tight bound        | Exact rate | MergeSort = Î˜(n log n) |
| **o**    | Strict upper bound | â€”          | n = o(nÂ²)              |
| **Ï‰**    | Strict lower bound | â€”          | nÂ² = Ï‰(n)              |

---

## ðŸ”¹ Interview Tips

* FAANG interviews typically ask:

  * â€œWhatâ€™s the **worst-case complexity**?â€ â†’ **Big-O**
  * â€œWhatâ€™s the **best-case complexity**?â€ â†’ **Big-Omega**
  * â€œWhatâ€™s the **average case**?â€ â†’ Usually same as expected case (often Î˜).
* Be prepared to compare algorithms by growth rates.
* Common trick: Ask about difference between **O** and **Î˜**.

---

## ðŸ”¹ Practice Problems

* [GFG â€“ Asymptotic Notations](https://www.geeksforgeeks.org/analysis-of-algorithms-asymptotic-analysis/)
* [LeetCode Discuss â€“ Time Complexity of Algorithms](https://leetcode.com/discuss/general-discussion/105039/time-complexity-of-common-leetcode-problems)
* GATE PYQs on asymptotic notations.

---

âœ… **Key Takeaway:**
Asymptotic notations abstract away constants and lower-order terms, letting us focus on **scalability** of algorithms.

---
